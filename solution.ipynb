{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Easy 21 Assignment solution\n",
    "This notebook is presenting the solution to the course assigment for David Silver's Reinforcement learning course at UCL.  \n",
    "\n",
    "You can find the course here: https://www.davidsilver.uk/teaching/   \n",
    "The assigment questions are here: https://www.davidsilver.uk/wp-content/uploads/2020/03/Easy21-Johannes.pdf\n",
    "\n",
    "This notebook does not contain the code, if you are interested in the implementation you can check out the Python script files in this folder. Utils.py contains the plotting scripts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Rules of the game\n",
    ">• The game is played with an infinite deck of cards (i.e. cards are sampled\n",
    "with replacement)  \n",
    "• Each draw from the deck results in a value between 1 and 10 (uniformly\n",
    "distributed) with a colour of red (probability 1/3) or black (probability\n",
    "2/3).  \n",
    "• There are no aces or picture (face) cards in this game  \n",
    "• At the start of the game both the player and the dealer draw one black\n",
    "card (fully observed)  \n",
    "• Each turn the player may either stick or hit  \n",
    "• If the player hits then she draws another card from the deck  \n",
    "• If the player sticks she receives no further cards  \n",
    "• The values of the player’s cards are added (black cards) or subtracted (red\n",
    "cards)  \n",
    "• If the player’s sum exceeds 21, or becomes less than 1, then she “goes\n",
    "bust” and loses the game (reward -1)  \n",
    "• If the player sticks then the dealer starts taking turns. The dealer always\n",
    "sticks on any sum of 17 or greater, and hits otherwise. If the dealer goes\n",
    "bust, then the player wins; otherwise, the outcome – win (reward +1),\n",
    "lose (reward -1), or draw (reward 0) – is the player with the largest sum.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Q1: Implementation of Easy21\n",
    ">You should write an environment that implements the game Easy21. Specifically, write a function, named step, which takes as input a state s (dealer’s first\n",
    "card 1–10 and the player’s sum 1–21), and an action a (hit or stick), and returns\n",
    "a sample of the next state s\n",
    "0\n",
    "(which may be terminal if the game is finished)\n",
    "and reward r. We will be using this environment for model-free reinforcement\n",
    "learning, and you should not explicitly represent the transition matrix for the\n",
    "MDP. There is no discounting (γ = 1). You should treat the dealer’s moves as\n",
    "part of the environment, i.e. calling step with a stick action will play out the\n",
    "dealer’s cards and return the final reward and terminal state.\n",
    "\n",
    "The environment is implemented in [q1_environment.py](q1_environment.py)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Q2: Monte-Carlo Control in Easy21\n",
    "\n",
    ">Apply Monte-Carlo control to Easy21. Initialise the value function to zero. Use\n",
    "a time-varying scalar step-size of αt = 1/N(st, at) and an epsilon-greedy exploration\n",
    "strategy with epsilon_t = N0/(N0 + N(st)), where N0 = 100 is a constant, N(s) is\n",
    "the number of times that state s has been visited, and N(s, a) is the number\n",
    "of times that action a has been selected from state s. Feel free to choose an\n",
    "alternative value for N0, if it helps producing better results. Plot the optimal\n",
    "value function V\n",
    "∗\n",
    "(s) = max_a Q∗\n",
    "(s, a) using similar axes to the following figure\n",
    "taken from Sutton and Barto’s Blackjack example\n",
    "\n",
    "The algorithm is implemented in [q2_monte_carlo_control.py](q2_monte_carlo_control.py)\n",
    "\n",
    "The optimal value function after 1000000 is the following:  \n",
    "\n",
    "![MC value function](mc_control_results/MC_Control_1000000.png)\n",
    "\n",
    "We can also look at a gif on how this was built:  \n",
    "\n",
    "![MC value function gif](mc_control_results/MC_Control.gif)\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The win rates look like this:  \n",
    "\n",
    "![Win rates for MC](mc_control_results/MC_Control_win_rate.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Q3: TD Learning in Easy21\n",
    "\n",
    ">Implement Sarsa(λ) in 21s. Initialise the value function to zero. Use the same\n",
    "step-size and exploration schedules as in the previous section. Run the algorithm\n",
    "with parameter values λ ∈ {0, 0.1, 0.2, ..., 1}. Stop each run after 1000 episodes and report the mean-squared error sum(\n",
    "Q(s, a) − Q∗\n",
    "(s, a))^2 over all states s\n",
    "and actions a, comparing the true values Q∗\n",
    "(s, a) computed in the previous\n",
    "section with the estimated values Q(s, a) computed by Sarsa. Plot the meansquared error against λ. For λ = 0 and λ = 1 only, plot the learning curve of\n",
    "mean-squared error against episode number\n",
    "\n",
    "The algorithm is implemented in [q3_sarsa.py](q3_sarsa.py)\n",
    "\n",
    "I have found that the MSE after 1000 episodes is not convering yet - I have plotted over 10000 episodes to show this, along with eventual decrease of the MSE. For lambda = 1 however we never seem to go below the initial MSE where we initialised Q at 0, which is a bit sad. More training would have probably helped it. Lambda = 1 seems to work the best.\n",
    "\n",
    "![MSE for lambdas](sarsa_results_assignment/sarsa_mse_for_lambdas.png)    \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "from IPython.display import Video, Image, HTML, display\n",
    "# read images\n",
    "img_A = 'sarsa_results_assignment/sarsa_mse_learning_0.png'\n",
    "img_B = 'sarsa_results_assignment/sarsa_mse_learning_1.png'\n",
    "\n",
    "HTML(f\"\"\"\n",
    "    <div class=\"row\">\n",
    "            <img src={img_A} style=\"width:45%\"> </img>\n",
    "            <img src={img_B} style=\"width:45%\"> </img>\n",
    "    </div>\n",
    "    \"\"\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <div class=\"row\">\n            <img src=sarsa_results_assignment/sarsa_mse_learning_0.png style=\"width:45%\"> </img>\n            <img src=sarsa_results_assignment/sarsa_mse_learning_1.png style=\"width:45%\"> </img>\n    </div>\n    "
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "source": [
    "I have also plotted the win rates and value function evolution out of curiosity:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <div class=\"row\">\n            <img src=sarsa_results_0/sarsa_win_rate.png style=\"width:45%\"> </img>\n            <img src=sarsa_results_1/sarsa_win_rate.png style=\"width:45%\"> </img>\n            <img src=sarsa_results_0/Sarsa_control_0.gif style=\"width:45%\"> </img>\n            <img src=sarsa_results_1/Sarsa_control_1.gif style=\"width:45%\"> </img>\n    </div>\n    "
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "from IPython.display import Video, Image, HTML, display\n",
    "# read images\n",
    "img_A = 'sarsa_results_0/sarsa_win_rate.png'\n",
    "img_B = 'sarsa_results_1/sarsa_win_rate.png'\n",
    "gif_A = \"sarsa_results_0/Sarsa_control_0.gif\"\n",
    "gif_B = \"sarsa_results_1/Sarsa_control_1.gif\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "    <div class=\"row\">\n",
    "            <img src={img_A} style=\"width:45%\"> </img>\n",
    "            <img src={img_B} style=\"width:45%\"> </img>\n",
    "            <img src={gif_A} style=\"width:45%\"> </img>\n",
    "            <img src={gif_B} style=\"width:45%\"> </img>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Q4: Linear Function Approximation in Easy21\n",
    "\n",
    ">We now consider a simple value function approximator using coarse coding. Use\n",
    "a binary feature vector φ(s, a) with 3 ∗ 6 ∗ 2 = 36 features. Each binary feature\n",
    "has a value of 1 iff (s, a) lies within the cuboid of state-space corresponding to\n",
    "that feature, and the action corresponding to that feature. The cuboids have\n",
    "the following overlapping intervals:  \n",
    "dealer(s) = {[1, 4], [4, 7], [7, 10]}  \n",
    "player(s) = {[1, 6], [4, 9], [7, 12], [10, 15], [13, 18], [16, 21]}  \n",
    "a = {hit, stick}  \n",
    "where  \n",
    "• dealer(s) is the value of the dealer’s first card (1–10)  \n",
    "• sum(s) is the sum of the player’s cards (1–21)  \n",
    "Repeat the Sarsa(λ) experiment from the previous section, but using linear\n",
    "value function approximation Q(s, a) = φ(s, a)\n",
    ">θ. Use a constant exploration\n",
    "of epsilon = 0.05 and a constant step-size of 0.01. Plot the mean-squared error against\n",
    "λ. For λ = 0 and λ = 1 only, plot the learning curve of mean-squared error\n",
    "against episode number.\n",
    "\n",
    "\n",
    "The alogithm is implemented in [q4_linear_fn_approx.py](q4_linear_fn_approx.py)\n",
    "I kept the 10000 episodes for the following plots. \n",
    "\n",
    "![Mse for different lambdas](linear_approx_assignment/sarsa_mse_for_lambdas.png)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <div class=\"row\">\n            <img src=linear_approx_assignment/linear_approx_learning_0.png style=\"width:45%\"> </img>\n            <img src=linear_approx_assignment/linear_approx_learning_1.png style=\"width:45%\"> </img>\n    </div>\n    "
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "from IPython.display import Video, Image, HTML, display\n",
    "# read images\n",
    "img_A = 'linear_approx_assignment/linear_approx_learning_0.png'\n",
    "img_B = 'linear_approx_assignment/linear_approx_learning_1.png'\n",
    "\n",
    "HTML(f\"\"\"\n",
    "    <div class=\"row\">\n",
    "            <img src={img_A} style=\"width:45%\"> </img>\n",
    "            <img src={img_B} style=\"width:45%\"> </img>\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "I have also plotted the win rates and value function evolution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": "\n    <div class=\"row\">\n            <img src=linear_results_0/linear_win_rate.png style=\"width:45%\"> </img>\n            <img src=linear_results_1/linear_win_rate.png style=\"width:45%\"> </img>\n            <img src=linear_results_0/Linear_control_0.gif style=\"width:45%\"> </img>\n            <img src=linear_results_1/Linear_control_1.gif style=\"width:45%\"> </img>\n    </div>\n    "
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#hide \n",
    "{\n",
    "    \"tags\": [\n",
    "        \"hide_input\",\n",
    "    ]\n",
    "}\n",
    "from IPython.display import Video, Image, HTML, display\n",
    "# read images\n",
    "img_A = 'linear_results_0/linear_win_rate.png'\n",
    "img_B = 'linear_results_1/linear_win_rate.png'\n",
    "gif_A = \"linear_results_0/Linear_control_0.gif\"\n",
    "gif_B = \"linear_results_1/Linear_control_1.gif\"\n",
    "\n",
    "HTML(f\"\"\"\n",
    "    <div class=\"row\">\n",
    "            <img src={img_A} style=\"width:45%\"> </img>\n",
    "            <img src={img_B} style=\"width:45%\"> </img>\n",
    "            <img src={gif_A} style=\"width:45%\"> </img>\n",
    "            <img src={gif_B} style=\"width:45%\"> </img>\n",
    "    </div>\n",
    "    \"\"\")"
   ]
  },
  {
   "source": [
    "### Q5: Discussion\n",
    "\n",
    "** What are the pros and cons of bootstrapping in Easy21? **\n",
    " \n",
    "- Pros:  \n",
    "    - Bootstrapping lets us learn during the episode based on our current estimates, so we don't have to wait until the end. This reduces the variance since we get less noise. This could have led to faster learning too, but that's not apparent by looking at the win rates plots. \n",
    "    - In this game particularly, there are lots of states where we are not in one of the best states (having 20-21), but a \"mediocre\" score of 10-15, we could stay in that state for many steps by alternating black and red card, and then quickly could go into a very good state (21) or a very bad state (getting below 0). The value function is quite sharp in some places, so reducing the variance with bootstrapping is helpful.  \n",
    "- Cons:  \n",
    "    - Bootstrapping can introduce bias propagating from the initial guesses, which can lead to a self-reinforcing loop of biased value function resulting in slower convergence of the method, especially with function approximation where the value function is \"wrong\" by design. Theoretically, Sarsa control should converge for table lookup and chatter around the optimal value for linear function approximation. although it might diverge for non-linear function approximation.\n",
    "\n",
    "** Would you expect bootstrapping to help more in blackjack or Easy21?\n",
    "Why?  **\n",
    "\n",
    "- I would expect it to work better in Easy21, because of the longer trajectories and the higher variance introduced by the red cards decreasing the sum of the cards in our hand. \n",
    "\n",
    "**What are the pros and cons of function approximation in Easy21? ** \n",
    "\n",
    "- Pros:\n",
    "    - Computationally easier and faster to get the optimal value function (meaning optimal for the approximation), as we have fewer states to worry about. This could be very helpful for bigger state spaces and more complicated games. \n",
    "    - It allows us to learn about many states at once if we manage to group together similar states effectively, thus speeding up the learning. \n",
    "- Cons:\n",
    "    - If the function approximation is not similar or different enough for similar/different states, it could lead to us learning a completely off value function. \n",
    "    - Together with bootstrapping it could be very unstable, as we have too many estimates and approximations and using very little \"real\" information in our model. We could also observe the sawtooth-like plots above.\n",
    "    - Easy21 does not have a big enough state-space to justify function approximation.\n",
    "\n",
    "**How would you modify the function approximator suggested in this section\n",
    "to get better results in Easy21? **\n",
    "\n",
    "- One of the problems we can see in the final value function of the function approximation is that we are not getting the negative slope as the dealer's hand increases on the x axis. This is because of the 3 intervals we use for the dealer's score, so making more intervals or just using the actual numbers instead of these intervals would make our approximation more effective. \n",
    "\n",
    "- Another problem is that we're not getting the sharp positive slope on the Y axis when we increase our hand from 19 to 21. This is because we can't distinguish between these states without current approximation, as we only have the \\[16-21\\] slot for them. Making 19, 20 and 21 separate categories would help this.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}